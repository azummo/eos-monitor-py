#!/usr/bin/env python

#from snotdaq import DataStream, Logger
#from minard.data import flush_to_redis, parse_cmos, unpack_index
from redis import Redis, StrictRedis
import socket
import threading
import time
import numpy as np
import os
import ctypes
from collections import defaultdict
from evb_types import *

from minard.timeseries import INTERVALS, EXPIRE, HASH_INTERVALS, HASH_EXPIRE
from minard.redistools import hmincrby, hdivk, setavgmax
import random


redis = Redis()

TRIGGER_NAMES = [
    '100L',  '100M',   '100H',
    '20',    '20LB',
    'ESUML', 'ESUMH',
    'OWLN',  'OWLEL',  'OWLEH',
    'PULGT', 'PRESCL', 'PED',
    'PONG',  'SYNC',
    'EXTA',  'EXT2',   'EXT3',
    'EXT4',  'EXT5',   'EXT6',
    'EXT7',  'EXT8',
    'SRAW',  'NCD',
    'SOFGT', 'MISS'
]

NDIG = 1

def recvall(s, n):
    data = bytearray()
    while len(data) < n:
        packet = s.recv(n - len(data))
        if not packet: return None
        data.extend(packet)
    return data

def post(host):
    """Posts the dispatcher name to redis every 20 seconds."""
    redis.set('dispatcher', 'localhost', ex=60)
    timer = threading.Timer(20.0, post, args=[host])
    timer.daemon = True
    timer.start()

class EventRecord(object):
    def __init__(self, record):
        self.avg = np.zeros(shape=(NDIG*16), dtype=int)
        self.std = np.zeros(shape=(NDIG*16), dtype=int)
        self.q = np.zeros(shape=(NDIG*16), dtype=int)
        self.t = np.zeros(shape=(NDIG*16), dtype=int)
        self.nhit = int(0)

        for idig in np.arange(NDIG):
            dig = record.caen[idig]
            for ich in np.arange(16):
                ch = dig.channels[ich]
                ns = min(500, dig.samples)
                chid = 16*idig + ich
                x = np.ctypeslib.as_array(ch.samples)

                avg = np.mean(x[:20])
                std = np.std(x[:20])
                self.avg[chid] = avg
                self.std[chid] = std

                xc = x - avg
                over_thresh = np.argwhere(xc < -6*std)

                if len(over_thresh) > 0:
                    q = -np.sum(xc[over_thresh])
                    t = over_thresh[0] * dig.ns_sample
                    self.nhit += 1

                    self.q[chid] = q
                    self.t[chid] = t


def flush_cache(cache, cache_set, cache_nhit, cache_pmt, time):
    # for docs on redis pipeline see http://redis.io/topics/pipelining
    p = redis.pipeline()

    for name, hash in list(cache.items()):
        if isinstance(hash, dict):
            keys = ['ts:%i:%i:%s' % (interval, time//interval, name)
                    for interval in INTERVALS]

            if len(hash) > 0:
                hmincrby(keys, hash, client=p)

                for key, interval in zip(keys,INTERVALS):
                    p.expire(key,interval*EXPIRE)
        else:
            for interval in INTERVALS:
                key = 'ts:%i:%i:%s' % (interval, time//interval, name)
                p.incrby(key, hash)
                p.expire(key,interval*EXPIRE)

    for interval in INTERVALS:
        for name, hash in list(cache_set.items()):
            key = 'ts:%i:%i:%s' % (interval, time//interval, name)
            if len(hash) > 0:
                p.hmset(key, hash)
                p.expire(key, interval*EXPIRE)

    keys = ['ts:%i:%i:occupancy:hits' % (interval, time//interval)
            for interval in HASH_INTERVALS]

    if len(cache_pmt) > 0:
        hmincrby(keys, cache_pmt, client=p)

    for interval in HASH_INTERVALS:
        key = 'ts:%i:%i:occupancy' % (interval, time//interval)
        p.incrby(key + ':count', cache['trig']['TOTAL'])
        # expire after just interval, because these will
        # occupancy will be set as hits/count
        p.expire(key + ':hits', interval)
        p.expire(key + ':count', interval)

        prev_key = 'ts:%i:%i:occupancy' % (interval,time//interval-1)
        if redis.incr(prev_key + ':lock') == 1:
            hdivk(prev_key, prev_key + ':hits', prev_key + ':count',
                  list(range(10240)), format='%.2g', client=p)
            keys = setavgmax(prev_key, client=p)
            for k in keys:
                p.expire(k, HASH_EXPIRE*interval)
            p.expire(prev_key, HASH_EXPIRE*interval)
            p.expire(prev_key + ':lock', interval)

    for trigger, cache in cache_nhit.items():
        if len(cache) > 0:
            # nhit distribution
            if len(cache) > 100:
                # if there are more than 100 events this second
                # randomly sample the nhit from 100 events
                cache = random.sample(cache,100)
            # see http://flask.pocoo.org/snippets/71/ for this design pattern
            p.lpush('ts:1:%i:nhit:%s' % (time, trigger), *cache)
            p.expire('ts:1:%i:nhit:%s' % (time, trigger), 3600)

    p.execute()


def pull(host, num_workers, worker):
    """Connects to the dispatcher and processes the dispatch stream."""
    post(host)
    #dispatcher = Dispatch(host)
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.bind(('localhost', 3491))
    s.listen(5)

    conn, addr = s.accept()
    print('connected from', addr)

    cache = {}
    cache['trig'] = defaultdict(int)
    cache['trig:nhit'] = defaultdict(int)
    cache['trig:charge'] = defaultdict(int)
    cache['DISPATCH_ORPHANS'] = 0
    cache_set = {}
    cache_set['trig'] = {}
    cache_nhit = defaultdict(list)
    cache_pmt = defaultdict(int)

    then = None

    while True:
        r = recvall(conn, ctypes.sizeof(CDABHeader))
        d = CDABHeader.from_buffer_copy(r)
        print('CDABHeader', d.type, d.len)
        if d.type != 1: continue

        r = recvall(conn, d.len)
        e = Event.from_buffer_copy(r)
        print('Event', e.id, e.type, hex(e.caen_status), hex(e.ptb_status))
        record = e

        now = int(time.time())
        if then is None:
            then = now

        if now > then:
            # flush data to redis every second
            flush_cache(cache, cache_set, cache_nhit, cache_pmt, then)

            p = redis.pipeline()
            for interval in INTERVALS:
                key = 'ts:%i:%i:heartbeat' % (interval, then//interval)
                p.setex(key,1,interval*EXPIRE)
            p.execute()

            cache['trig'].clear()
            cache['trig:nhit'].clear()
            cache['trig:charge'].clear()
            cache['DISPATCH_ORPHANS'] = 0
            cache_set['trig'].clear()
            cache_nhit.clear()
            cache_pmt.clear()
            then = now

        if record is None:
            # nothing to process, take a break
            #log.debug("timeout")
            time.sleep(0.01)
            continue

        rec = EventRecord(record)
    
        run = 0  #pev.RunNumber
        gtid = record.ptb.timestamp & 0xffffffff  #pev.TriggerCardData.BcGT
        print(type(gtid))

        #if gtid % num_workers != worker:
        #    continue

        nhit = rec.nhit  #pev.NPmtHit
        subrun = 0  #pev.DaqStatus # seriously :)
        trig = record.ptb.trigger_word  # unpack_trigger_type(pev)

        qhs_sum = int(np.sum(rec.q))
        for i in range(len(rec.q)): cache_pmt[i] += 1

        # orphan
        if trig == 0 and gtid == 0:
            cache['DISPATCH_ORPHANS'] += nhit
            continue

        cache_nhit['all'].append(nhit)

        cache['trig']['TOTAL'] += 1
        cache['trig:nhit']['TOTAL'] += nhit
        cache['trig:charge']['TOTAL'] += qhs_sum
        cache_set['trig']['run'] = run
        cache_set['trig']['subrun'] = subrun
        cache_set['trig']['gtid'] = gtid

        # no trigger word, but not an orphan
        if trig == 0:
            cache['trig']['NONE'] += 1
            cache['trig:nhit']['NONE'] += nhit
            cache['trig:charge']['NONE'] += qhs_sum
            cache_nhit['NONE'].append(nhit)
            continue

        for i, name in enumerate(TRIGGER_NAMES):
            if trig & (1 << i):
                cache['trig'][i] += 1
                cache['trig:nhit'][i] += nhit
                cache['trig:charge'][i] += qhs_sum
                cache_nhit[name].append(nhit)

        # unused bit in MTC word marks front-end polling
        #polling = pev.TriggerCardData.Unused1

        #if polling:
        #    cache['trig']['polling'] += 1
        #    cache['trig:nhit']['polling'] += nhit
        #    cache['trig:charge']['polling'] += qhs_sum
        #    cache_nhit['polling'].append(nhit)


#def daemonize():
#    if os.fork() != 0:
#        os._exit(0)
#
#    os.setsid()
#
#    f = open('/dev/null', 'w')
#
#    fd = f.fileno()
#
#    os.dup2(fd, 0)
#    os.dup2(fd, 1)
#    os.dup2(fd, 2)

if __name__ == '__main__':
    import sys
    import argparse

    parser = argparse.ArgumentParser(description='Process SNO+ events from a dispatch stream and update redis')
    parser.add_argument('--host', default='localhost',
                        help='hostname of the dispatcher')
    parser.add_argument('-n', '--num-workers', type=int, default=1,
                        help='total number of workers running')
    parser.add_argument('-w', '--worker', type=int, default=0,
                        help='the worker number. This means the script will only process GTIDs which are equal to the worker number mod the total number of workers.')
    parser.add_argument("--logfile", help="filename for log file",
                        default=None)
    parser.add_argument("-d", "--daemonize", action="store_true",
            default=False)
    parser.add_argument('--loglevel',
                        help="logging level (debug, verbose, notice, warning)",
                        default='notice')
    args = parser.parse_args()

    #log.set_verbosity(args.loglevel)

    #if args.logfile:
    #    log.set_logfile(args.logfile)

#    if args.daemonize:
#        daemonize()

    if args.worker >= args.num_workers:
        #log.warn("worker number must be less than the number of workers")
        sys.exit(1)

    while True:
        #try:
        pull(args.host, args.num_workers, args.worker)
        #except Exception as e:
        #    #log.warn("unhandled exception: %s" % str(e))
        #    print(str(e))
        #    time.sleep(60)
        #    continue

